{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification with an RNN 20211013.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNrY9+VzvhCXwb56OcszzCN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/space-owner/Tensorflow-2/blob/main/Text%20Classification%20with%20RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ruz6_r3Rf9J8"
      },
      "source": [
        "### ***Text Classification with an RNN***\n",
        "This post is **based on the Tensorflow tutorial** for study purposes. [Link](https://www.tensorflow.org/text/tutorials/text_classification_rnn)\n",
        "\n",
        "***Learning Point:***\n",
        "- **```Many-to-One Architecture```**\n",
        "- **```tf.keras.Bidirectional()```**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oOCkS5_xCs5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fbf8a39-aab6-4005-aae4-7a0799db9028"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "print(\">>> tf.version =\", tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> tf.version = 2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zA93kDizNyx"
      },
      "source": [
        "def plot_graphs(history, metric):\n",
        "    plt.plot(history.history[metric])\n",
        "    plt.plot(history.history['val_'+metric], '')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(metric)\n",
        "    plt.legend([metric, 'val_'+metric])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJKro-lZzpOg",
        "outputId": "b2dd223d-0c31-4be0-aa9c-a2e6c39ecfbe"
      },
      "source": [
        "dataset, info = tfds.load(\n",
        "    'imdb_reviews', with_info=True, as_supervised=True\n",
        ")\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "train_dataset.element_spec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDownloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete1JN2TF/imdb_reviews-train.tfrecord\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete1JN2TF/imdb_reviews-test.tfrecord\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete1JN2TF/imdb_reviews-unsupervised.tfrecord\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Dataset is using deprecated text encoder API which will be removed soon. Please use the plain_text version of the dataset and migrate to `tensorflow_text`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vubqpT3O0NIZ",
        "outputId": "1ba1ef9d-7829-49c2-9ff9-ca0e3cd96c76"
      },
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "    print(\">>> text =\", example.numpy())\n",
        "    print(\">>> label =\", label.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> text = b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
            ">>> label = 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouQnCKu70pcv"
      },
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-_2Tcq1wtFy"
      },
      "source": [
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "\n",
        "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfM33Aau1Etf",
        "outputId": "1ad6bd54-4b6c-434a-e802-01d5ce96d5e3"
      },
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "    print(\">>> text =\", example.numpy()[:3])\n",
        "    print(\">>> label =\", label.numpy()[:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> text = [b'I\\'m on the opposite end of the previous comment.<br /><br />First of all, I don\\'t think this was intended to be a straight sequel to \"The Jerk\". I mean, it\\'s not titled \"The Jerk 2\"... it\\'s \"The Jerk, Too\", which leads me to believe that while a lot of the character names are the same, it actually revolves around a completely different person.<br /><br />Think about it: Virtually no connection to the previous movie, other than character names; a totally different story; different cast; and the fact that it\\'s a partial musical.<br /><br />I say give this movie some credit. It does have plenty of laughs in it.. Mark Blankfield at his prime.'\n",
            " b\"But, lets face it... it got a few nostalgic sighs out of me.<br /><br />The show is just so consistently great that it is allowed to have a few hiccups. I get a new season, and just power through them like I have 2-days to live. I like the idea of wrapping it up, but it was much more of an end of season episode which would explain the following:<br /><br />Dr.Cox isn't supposed to be bald for a couple more episodes, only explanation I can think of is they changed the rotation of the episodes or had to re-shoot the beginning.<br /><br />and that my friends, is why the hell cox is bald.<br /><br />Anyways, the show is awesome...bring on the 7th season.\"\n",
            " b'\"Voodoo Academy\" features an \"Academy\" like no other, one that houses only six male students in one bedroom. These teenage guys are instructed in religion by a sinister young priest, who enjoys tormenting and comforting them simultaneously. The sole administrator of this \"Academy\" is a young and seductive headmistress, and she retains her handsome charges on a short leash, so to speak.<br /><br />Sexual overtones abound, and the director obviously has high regard for young male bodies. These young actors occasionally strip down to their designer underwear to sneak about the \"Academy,\" and their sexuality is the entire focus of the movie. If you\\'re not interested in the male form -- stay away!<br /><br />Burdened by weak and awkward dialogue, this low-budget exploitation piece just stumbles along with a few laughable special effects tossed in between the yawns. The mood is claustrophobic, with tediously long takes, a handful of cheap sets and few costume changes. These visual elements come interspersed with seemingly unending sequences of banal dialogue, intended as character and plot development. It gives one the feeling it was filmed in three days...']\n",
            ">>> label = [1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC-eVx5Yv0MZ"
      },
      "source": [
        "VOCAB_SIZE = 1000\n",
        "\n",
        "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE\n",
        ")\n",
        "encoder.adapt(\n",
        "    train_dataset.map(lambda text, label: text)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH2U0_XY1oFx",
        "outputId": "7dbacf75-a78b-4000-8d82-708a6250742c"
      },
      "source": [
        "vocab = np.array(encoder.get_vocabulary())\n",
        "\n",
        "print(\">>> vocab example =\", vocab[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> vocab example = ['' '[UNK]' 'the' 'and' 'a' 'of' 'to' 'is' 'in' 'it' 'i' 'this' 'that'\n",
            " 'br' 'was' 'as' 'for' 'with' 'movie' 'but']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uTi80Vn19-U",
        "outputId": "79305928-0573-4563-f8c7-0520a3237af3"
      },
      "source": [
        "encoded_example = encoder(example)[:3].numpy()\n",
        "\n",
        "print(\">>> encoded_example =\", encoded_example)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> encoded_example = [[142  21   2 ...   0   0   0]\n",
            " [ 19 599 403 ...   0   0   0]\n",
            " [  1   1 908 ...   0   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1zWewXRvYBL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13b50811-e55d-4597-ce40-cd30c57b2437"
      },
      "source": [
        "for n in range(3):\n",
        "    print(\"Original: \", example[n].numpy())\n",
        "    print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  b'I\\'m on the opposite end of the previous comment.<br /><br />First of all, I don\\'t think this was intended to be a straight sequel to \"The Jerk\". I mean, it\\'s not titled \"The Jerk 2\"... it\\'s \"The Jerk, Too\", which leads me to believe that while a lot of the character names are the same, it actually revolves around a completely different person.<br /><br />Think about it: Virtually no connection to the previous movie, other than character names; a totally different story; different cast; and the fact that it\\'s a partial musical.<br /><br />I say give this movie some credit. It does have plenty of laughs in it.. Mark Blankfield at his prime.'\n",
            "Round-trip:  im on the [UNK] end of the previous [UNK] br first of all i dont think this was [UNK] to be a straight sequel to the [UNK] i mean its not [UNK] the [UNK] 2 its the [UNK] too which leads me to believe that while a lot of the character [UNK] are the same it actually [UNK] around a completely different [UNK] br think about it [UNK] no [UNK] to the previous movie other than character [UNK] a totally different story different cast and the fact that its a [UNK] [UNK] br i say give this movie some [UNK] it does have plenty of laughs in it mark [UNK] at his [UNK]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
            "\n",
            "Original:  b\"But, lets face it... it got a few nostalgic sighs out of me.<br /><br />The show is just so consistently great that it is allowed to have a few hiccups. I get a new season, and just power through them like I have 2-days to live. I like the idea of wrapping it up, but it was much more of an end of season episode which would explain the following:<br /><br />Dr.Cox isn't supposed to be bald for a couple more episodes, only explanation I can think of is they changed the rotation of the episodes or had to re-shoot the beginning.<br /><br />and that my friends, is why the hell cox is bald.<br /><br />Anyways, the show is awesome...bring on the 7th season.\"\n",
            "Round-trip:  but lets face it it got a few [UNK] [UNK] out of [UNK] br the show is just so [UNK] great that it is [UNK] to have a few [UNK] i get a new season and just power through them like i have [UNK] to live i like the idea of [UNK] it up but it was much more of an end of season episode which would [UNK] the [UNK] br [UNK] isnt supposed to be [UNK] for a couple more episodes only [UNK] i can think of is they [UNK] the [UNK] of the episodes or had to [UNK] the [UNK] br and that my friends is why the hell [UNK] is [UNK] br [UNK] the show is [UNK] on the [UNK] season                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
            "\n",
            "Original:  b'\"Voodoo Academy\" features an \"Academy\" like no other, one that houses only six male students in one bedroom. These teenage guys are instructed in religion by a sinister young priest, who enjoys tormenting and comforting them simultaneously. The sole administrator of this \"Academy\" is a young and seductive headmistress, and she retains her handsome charges on a short leash, so to speak.<br /><br />Sexual overtones abound, and the director obviously has high regard for young male bodies. These young actors occasionally strip down to their designer underwear to sneak about the \"Academy,\" and their sexuality is the entire focus of the movie. If you\\'re not interested in the male form -- stay away!<br /><br />Burdened by weak and awkward dialogue, this low-budget exploitation piece just stumbles along with a few laughable special effects tossed in between the yawns. The mood is claustrophobic, with tediously long takes, a handful of cheap sets and few costume changes. These visual elements come interspersed with seemingly unending sequences of banal dialogue, intended as character and plot development. It gives one the feeling it was filmed in three days...'\n",
            "Round-trip:  [UNK] [UNK] features an [UNK] like no other one that [UNK] only [UNK] male [UNK] in one [UNK] these [UNK] guys are [UNK] in [UNK] by a [UNK] young [UNK] who [UNK] [UNK] and [UNK] them [UNK] the [UNK] [UNK] of this [UNK] is a young and [UNK] [UNK] and she [UNK] her [UNK] [UNK] on a short [UNK] so to [UNK] br sexual [UNK] [UNK] and the director obviously has high [UNK] for young male [UNK] these young actors [UNK] [UNK] down to their [UNK] [UNK] to [UNK] about the [UNK] and their [UNK] is the entire [UNK] of the movie if youre not interested in the male form stay [UNK] br [UNK] by weak and [UNK] dialogue this [UNK] [UNK] piece just [UNK] along with a few [UNK] special effects [UNK] in between the [UNK] the [UNK] is [UNK] with [UNK] long takes a [UNK] of cheap sets and few [UNK] [UNK] these [UNK] elements come [UNK] with [UNK] [UNK] sequences of [UNK] dialogue [UNK] as character and plot development it gives one the feeling it was filmed in three days                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNiNQffm16A_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d601297-df12-430e-ab8c-e993dadb7965"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()), output_dim=64, mask_zero=True\n",
        "    ),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "print(\"model.layers =\", [print(layer) for layer in model.layers])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<keras.layers.preprocessing.text_vectorization.TextVectorization object at 0x7f1c546b5050>\n",
            "<keras.layers.embeddings.Embedding object at 0x7f1c5470aa10>\n",
            "<keras.layers.wrappers.Bidirectional object at 0x7f1c5749a3d0>\n",
            "<keras.layers.core.Dense object at 0x7f1c53db4810>\n",
            "<keras.layers.core.Dense object at 0x7f1c53db4490>\n",
            "model.layers = [None, None, None, None, None]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1mJIlE7ezuI",
        "outputId": "547dc0d3-6703-4f32-95b1-fa292aa14356"
      },
      "source": [
        "print(\n",
        "    [layer.supports_masking for layer in model.layers]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[False, True, True, True, True]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51IcciCyvYIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2df9d13b-4b5a-43d2-fcce-1046c335b33e"
      },
      "source": [
        "sample_text = (\n",
        "    'The movie was cool. The animation and the graphics '\n",
        "    'were out of this world. I would recommend this movie.'\n",
        ")\n",
        "print(\">>> np.array([sample_text]) =\", np.array([sample_text]))\n",
        "\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "\n",
        "print(\">>> predictions =\", predictions[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> np.array([sample_text]) = ['The movie was cool. The animation and the graphics were out of this world. I would recommend this movie.']\n",
            ">>> predictions = [-0.00798043]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b94xtCyvYLA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d8c1e2c-7776-400f-ee7f-4c512b538083"
      },
      "source": [
        "padding = \"the \" * 2000\n",
        "\n",
        "print(\">>> np.array([sample_text, padding]) = \", np.array([sample_text, padding]))\n",
        "\n",
        "predictions = model.predict(np.array([sample_text, padding]))\n",
        "\n",
        "print(\">>> predictions =\", predictions[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> np.array([sample_text, padding]) =  ['The movie was cool. The animation and the graphics were out of this world. I would recommend this movie.'\n",
            " 'the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the ']\n",
            ">>> predictions = [-0.00798043]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Gj43gPfgVuQ"
      },
      "source": [
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcApNO8xge2w",
        "outputId": "b6d3c1be-4481-4851-bde1-73846b2b041b"
      },
      "source": [
        "history = model.fit(\n",
        "    train_dataset, \n",
        "    epochs=10,\n",
        "    validation_data=test_dataset,\n",
        "    validation_steps=30\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - 108s 253ms/step - loss: 0.6379 - accuracy: 0.5966 - val_loss: 0.5266 - val_accuracy: 0.7708\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 95s 240ms/step - loss: 0.4597 - accuracy: 0.7928 - val_loss: 0.3788 - val_accuracy: 0.8443\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 97s 245ms/step - loss: 0.3685 - accuracy: 0.8377 - val_loss: 0.3350 - val_accuracy: 0.8521\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 96s 243ms/step - loss: 0.3352 - accuracy: 0.8531 - val_loss: 0.3245 - val_accuracy: 0.8583\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 97s 245ms/step - loss: 0.3187 - accuracy: 0.8623 - val_loss: 0.3171 - val_accuracy: 0.8729\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 93s 235ms/step - loss: 0.3130 - accuracy: 0.8653 - val_loss: 0.3245 - val_accuracy: 0.8464\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 94s 237ms/step - loss: 0.3061 - accuracy: 0.8680 - val_loss: 0.2952 - val_accuracy: 0.8745\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 93s 235ms/step - loss: 0.3022 - accuracy: 0.8688 - val_loss: 0.3042 - val_accuracy: 0.8703\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 94s 238ms/step - loss: 0.3010 - accuracy: 0.8701 - val_loss: 0.3523 - val_accuracy: 0.8526\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 93s 235ms/step - loss: 0.2974 - accuracy: 0.8722 - val_loss: 0.3174 - val_accuracy: 0.8703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hbqs0UTlgkGZ",
        "outputId": "9eabaa42-be67-40d2-e45a-4e2514dc1174"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print('test_loss:', test_loss)\n",
        "print('test_ac:', test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "391/391 [==============================] - 48s 120ms/step - loss: 0.3231 - accuracy: 0.8617\n",
            "test_loss: 0.3231208324432373\n",
            "test_ac: 0.8616799712181091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k0LYAHnhJEF"
      },
      "source": [
        "### ***Stack Two or More LSTM Layers***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EHCFLNbgtBH",
        "outputId": "beaa382e-c12d-4e27-af0c-bfa2d8474a11"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=10,\n",
        "    validation_data=test_dataset,\n",
        "    validation_steps=30\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - 195s 455ms/step - loss: 0.6099 - accuracy: 0.6096 - val_loss: 0.4165 - val_accuracy: 0.8188\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 170s 433ms/step - loss: 0.3996 - accuracy: 0.8282 - val_loss: 0.3567 - val_accuracy: 0.8448\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 171s 434ms/step - loss: 0.3447 - accuracy: 0.8562 - val_loss: 0.3325 - val_accuracy: 0.8531\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 170s 433ms/step - loss: 0.3242 - accuracy: 0.8638 - val_loss: 0.3102 - val_accuracy: 0.8776\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 171s 435ms/step - loss: 0.3155 - accuracy: 0.8682 - val_loss: 0.3170 - val_accuracy: 0.8568\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 173s 440ms/step - loss: 0.3137 - accuracy: 0.8674 - val_loss: 0.3084 - val_accuracy: 0.8625\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 169s 428ms/step - loss: 0.3066 - accuracy: 0.8726 - val_loss: 0.3185 - val_accuracy: 0.8599\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 171s 434ms/step - loss: 0.3066 - accuracy: 0.8707 - val_loss: 0.3291 - val_accuracy: 0.8271\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 170s 431ms/step - loss: 0.3044 - accuracy: 0.8716 - val_loss: 0.3155 - val_accuracy: 0.8479\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 170s 432ms/step - loss: 0.3030 - accuracy: 0.8717 - val_loss: 0.3012 - val_accuracy: 0.8687\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0tjpP19vYNe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c83f510e-23c5-481d-9d1c-f0579332e00a"
      },
      "source": [
        "sample_text = (\n",
        "    'The movie was not good. The animation and the graphics '\n",
        "    'were terrible. I would not recommend this movie.'\n",
        ")\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(\">>> predictions = \", predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> predictions =  [[-1.5732746]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sc0CSr0unB0e",
        "outputId": "b6f0a6ea-15e6-40e6-8519-93a8c98fdc2e"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print('test_loss:', test_loss)\n",
        "print('test_ac:', test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "391/391 [==============================] - 83s 210ms/step - loss: 0.3151 - accuracy: 0.8546\n",
            "test_loss: 0.3151448965072632\n",
            "test_ac: 0.854640007019043\n"
          ]
        }
      ]
    }
  ]
}